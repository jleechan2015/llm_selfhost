#!/usr/bin/env python3
"""
Complete Tool Execution Test Suite for claude-local
Tests actual tool execution through logged proxy interactions
"""

import os
import sys
import time
import subprocess
import tempfile
import json
from pathlib import Path
from datetime import datetime

def log_test(message):
    """Log test messages with timestamp"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"üè† [{timestamp}] {message}")

def run_claude_local_test(prompt, test_name, timeout=60):
    """Run a claude-local command and capture all output"""
    log_test(f"Starting test: {test_name}")
    
    # Create temp log file for this test
    log_file = f"/tmp/claude_local_test_{test_name.replace(' ', '_')}.log"
    
    try:
        # Run claude-local with output redirection
        cmd = f"./claude-local -p '{prompt}' > {log_file} 2>&1"
        log_test(f"Executing: {cmd}")
        
        result = subprocess.run(
            cmd, 
            shell=True, 
            timeout=timeout,
            cwd="/home/jleechan/projects/claude_llm_proxy"
        )
        
        # Read the output
        with open(log_file, 'r') as f:
            output = f.read()
        
        log_test(f"Test {test_name} completed with exit code: {result.returncode}")
        return {
            'exit_code': result.returncode,
            'output': output,
            'log_file': log_file
        }
        
    except subprocess.TimeoutExpired:
        log_test(f"Test {test_name} timed out after {timeout} seconds")
        return {
            'exit_code': -1,
            'output': 'TIMEOUT',
            'log_file': log_file
        }
    except Exception as e:
        log_test(f"Test {test_name} failed with error: {str(e)}")
        return {
            'exit_code': -2,
            'output': f'ERROR: {str(e)}',
            'log_file': log_file
        }

def verify_file_creation(filename, expected_content=None):
    """Verify a file was actually created"""
    filepath = f"/home/jleechan/projects/claude_llm_proxy/{filename}"
    
    if os.path.exists(filepath):
        log_test(f"‚úÖ File {filename} was created")
        if expected_content:
            with open(filepath, 'r') as f:
                actual_content = f.read().strip()
            if expected_content in actual_content:
                log_test(f"‚úÖ File {filename} contains expected content")
                return True
            else:
                log_test(f"‚ùå File {filename} content mismatch. Expected: '{expected_content}', Got: '{actual_content}'")
                return False
        return True
    else:
        log_test(f"‚ùå File {filename} was NOT created")
        return False

def analyze_logs_for_tool_execution(log_file):
    """Analyze logs to verify tool execution occurred"""
    if not os.path.exists(log_file):
        log_test(f"‚ùå Log file {log_file} not found")
        return False
    
    with open(log_file, 'r') as f:
        content = f.read()
    
    # Look for tool execution indicators
    tool_indicators = [
        "LOCAL TOOL EXECUTION STARTED",
        "Executing bash:",
        "File operation:",
        "LOCAL TOOL EXECUTION COMPLETED"
    ]
    
    found_indicators = []
    for indicator in tool_indicators:
        if indicator in content:
            found_indicators.append(indicator)
            log_test(f"‚úÖ Found tool execution indicator: {indicator}")
        else:
            log_test(f"‚ùå Missing tool execution indicator: {indicator}")
    
    if len(found_indicators) >= 2:  # At least START and some execution
        log_test("‚úÖ Tool execution confirmed through logs")
        return True
    else:
        log_test("‚ùå No clear evidence of tool execution in logs")
        return False

def check_lm_studio():
    """Check if LM Studio is available"""
    import requests
    try:
        # Check for LM Studio on Windows host
        host_ip = subprocess.check_output(
            "ip route show | grep -i default | awk '{ print $3 }' | head -1", 
            shell=True
        ).decode().strip()
        
        response = requests.get(f"http://{host_ip}:1234/v1/models", timeout=5)
        if response.status_code == 200:
            log_test(f"‚úÖ LM Studio is running on {host_ip}:1234")
            return True
        else:
            log_test(f"‚ùå LM Studio not responding on {host_ip}:1234")
            return False
    except Exception as e:
        log_test(f"‚ùå Cannot connect to LM Studio: {str(e)}")
        return False

def main():
    """Run comprehensive claude-local tool execution tests"""
    log_test("=== Claude-Local Tool Execution Test Suite ===")
    
    # Check prerequisites
    if not check_lm_studio():
        log_test("‚ö†Ô∏è  Warning: LM Studio not available. Tests may fail.")
    
    # Clean up from previous tests
    test_files = [
        "test_local_file_create.txt",
        "test_local_file_edit.txt", 
        "test_local_bash_output.txt"
    ]
    
    for file in test_files:
        filepath = f"/home/jleechan/projects/claude_llm_proxy/{file}"
        if os.path.exists(filepath):
            os.remove(filepath)
            log_test(f"üßπ Cleaned up existing file: {file}")
    
    # Test cases
    tests = [
        {
            'name': 'file_creation',
            'prompt': 'Create a file called test_local_file_create.txt with the content "Hello from claude-local tools"',
            'verify_file': 'test_local_file_create.txt',
            'expected_content': 'Hello from claude-local tools'
        },
        {
            'name': 'bash_execution',
            'prompt': 'Run the command "echo \'Local bash test\' > test_local_bash_output.txt"',
            'verify_file': 'test_local_bash_output.txt',
            'expected_content': 'Local bash test'
        },
        {
            'name': 'file_editing',
            'prompt': 'Create a file test_local_file_edit.txt with "original content", then edit it to say "local edited content"',
            'verify_file': 'test_local_file_edit.txt',
            'expected_content': 'local edited content'
        }
    ]
    
    results = []
    
    for test in tests:
        log_test(f"\n--- Running Test: {test['name']} ---")
        
        # Run the test
        result = run_claude_local_test(test['prompt'], test['name'])
        
        # Verify file creation
        file_created = verify_file_creation(test['verify_file'], test.get('expected_content'))
        
        # Analyze logs for tool execution
        tools_executed = analyze_logs_for_tool_execution(result['log_file'])
        
        # Store results
        test_result = {
            'name': test['name'],
            'exit_code': result['exit_code'],
            'file_created': file_created,
            'tools_executed': tools_executed,
            'log_file': result['log_file'],
            'success': file_created and tools_executed and result['exit_code'] == 0
        }
        
        results.append(test_result)
        
        if test_result['success']:
            log_test(f"‚úÖ Test {test['name']} PASSED")
        else:
            log_test(f"‚ùå Test {test['name']} FAILED")
    
    # Final report
    log_test("\n=== FINAL RESULTS ===")
    passed = sum(1 for r in results if r['success'])
    total = len(results)
    
    log_test(f"Tests passed: {passed}/{total}")
    
    for result in results:
        status = "‚úÖ PASS" if result['success'] else "‚ùå FAIL"
        log_test(f"{status} {result['name']}: exit_code={result['exit_code']}, file_created={result['file_created']}, tools_executed={result['tools_executed']}")
    
    # Save detailed results
    results_file = "/tmp/claude_local_test_results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    log_test(f"Detailed results saved to: {results_file}")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)