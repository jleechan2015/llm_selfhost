# Gemini Project Configuration

## Project Overview

This project is a Python-based proxy server for various Large Language Models (LLMs), with a command-line interface (`llm-proxy.js`) and a FastAPI backend. It's designed to work with the Claude CLI, providing a unified interface for different backends like Cerebras, vast.ai, and local Ollama instances.

## Commands

### Installation

To install the project dependencies, use the `install.sh` script. This will install system dependencies, Python packages from `requirements.txt`, and Ollama for local model support.

```bash
./install.sh
```

### Running the Proxy

The main script for running the proxy is `llm_proxy_start.sh`. You can specify the backend to use with flags:

*   **Cerebras:** `bash ./llm_proxy_start.sh --cerebras`
*   **Vast.ai:** `bash ./llm_proxy_start.sh --vast`
*   **Local:** `bash ./llm_proxy_start.sh --local`

### Testing

To run all tests, use the `run_all_tests.sh` script in the `tests` directory:

```bash
(cd tests && ./run_all_tests.sh)
```

Individual test scripts are also available in the `tests` directory (e.g., `test_claude_local.sh`).

## File Structure

*   `*.py`: Python source files for the proxy backends and FastAPI application.
*   `*.sh`: Shell scripts for installation, startup, and testing.
*   `requirements.txt`: Python dependencies.
*   `.llmrc.json`: Configuration file for the LLM proxy (generated by `llm_proxy_start.sh`).
*   `bin/llm-proxy.js`: JavaScript-based command-line interface for the proxy.
*   `tests/`: Test scripts.
*   `claude_env/`: Python virtual environment.

## Dependencies

*   **Python:** FastAPI, Uvicorn, Requests, Redis (optional).
*   **JavaScript:** The `llm-proxy.js` script suggests a Node.js environment, but `package.json` is missing.
*   **Other:** Ollama for local models, `vastai` CLI for vast.ai integration.
