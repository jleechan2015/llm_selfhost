#!/bin/bash
# Claude CLI with Local LM Studio Integration
# Auto-discovers LM Studio on Windows host from WSL
# Usage: ./claude-local [claude options]

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}üè† Claude CLI - Local LM Studio Mode${NC}"
echo "========================================="

# Configuration
LM_STUDIO_PORT="${LM_STUDIO_PORT:-1234}"
LM_STUDIO_MODEL="${LM_STUDIO_MODEL:-qwen/qwen3-coder-30b}"

# Discover Windows host IP from WSL
discover_host() {
    # Check if we're in WSL
    if ! grep -q microsoft /proc/version 2>/dev/null; then
        echo "localhost"
        return 0
    fi
    
    # Get default gateway (Windows host IP)
    HOST_IP=$(ip route show | grep -i default | awk '{ print $3 }' | head -1)
    
    if [ -z "$HOST_IP" ]; then
        echo "localhost"
        return 1
    fi
    
    echo "$HOST_IP"
    return 0
}

# Display host discovery results
show_host_discovery() {
    echo -e "${BLUE}üîç Discovering Windows host from WSL...${NC}"
    
    HOST_IP=$(discover_host)
    
    if [ "$HOST_IP" == "localhost" ]; then
        if grep -q microsoft /proc/version 2>/dev/null; then
            echo -e "${YELLOW}‚ö†Ô∏è  Could not discover Windows host IP, using localhost${NC}"
        else
            echo -e "${YELLOW}‚ö†Ô∏è  Not running in WSL, using localhost${NC}"
        fi
    else
        echo -e "${GREEN}‚úÖ Found Windows host: $HOST_IP${NC}"
    fi
}

# Check if LM Studio is running and accessible
check_lm_studio() {
    local host=$1
    local port=$2
    
    echo -e "${BLUE}üîç Checking LM Studio at ${host}:${port}...${NC}"
    
    # Test basic connectivity
    echo -e "${BLUE}‚è≥ Testing connection to http://${host}:${port}/v1/models${NC}"
    if ! curl -s --connect-timeout 15 --max-time 30 "http://${host}:${port}/v1/models" >/dev/null 2>&1; then
        echo -e "${RED}‚ùå Cannot connect to LM Studio${NC}"
        echo -e "${YELLOW}üí° Testing direct connectivity...${NC}"
        curl -v --connect-timeout 5 "http://${host}:${port}/v1/models" 2>&1 | head -5
        return 1
    fi
    
    # Get models list
    MODELS_RESPONSE=$(curl -s --connect-timeout 10 "http://${host}:${port}/v1/models" 2>/dev/null)
    
    if [ $? -ne 0 ] || [ -z "$MODELS_RESPONSE" ]; then
        echo -e "${RED}‚ùå LM Studio not responding properly${NC}"
        return 1
    fi
    
    # Check if our expected model is loaded
    if echo "$MODELS_RESPONSE" | grep -q "qwen"; then
        MODEL_NAME=$(echo "$MODELS_RESPONSE" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    models = [m['id'] for m in data.get('data', [])]
    qwen_models = [m for m in models if 'qwen' in m.lower()]
    print(qwen_models[0] if qwen_models else 'unknown')
except:
    print('unknown')
" 2>/dev/null)
        
        echo -e "${GREEN}‚úÖ LM Studio is running${NC}"
        echo -e "${GREEN}üì± Available model: $MODEL_NAME${NC}"
        return 0
    else
        echo -e "${YELLOW}‚ö†Ô∏è  LM Studio running but no Qwen model loaded${NC}"
        echo -e "${BLUE}Available models:${NC}"
        echo "$MODELS_RESPONSE" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    for model in data.get('data', []):
        print(f\"  - {model['id']}\")
except:
    print('  Unable to parse models')
" 2>/dev/null
        return 1
    fi
}

# Test LM Studio connection with a simple query
test_lm_studio() {
    local host=$1
    local port=$2
    local model=$3
    
    echo -e "${BLUE}üß™ Testing LM Studio with simple query...${NC}"
    
    # Simple test query
    local test_response=$(curl -s --connect-timeout 15 \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer lm-studio" \
        -d "{
            \"model\": \"$model\",
            \"messages\": [{\"role\": \"user\", \"content\": \"Say 'Hello from LM Studio!' and nothing else.\"}],
            \"max_tokens\": 20,
            \"temperature\": 0.1
        }" \
        "http://${host}:${port}/v1/chat/completions" 2>/dev/null)
    
    if [ $? -eq 0 ] && echo "$test_response" | grep -q "Hello from LM Studio"; then
        echo -e "${GREEN}‚úÖ LM Studio test successful${NC}"
        return 0
    else
        echo -e "${YELLOW}‚ö†Ô∏è  LM Studio test failed or unexpected response${NC}"
        echo "Response: $test_response"
        return 1
    fi
}

# Create local proxy for Claude CLI integration
start_local_proxy() {
    local host=$1
    local port=$2
    local model=$3
    
    echo -e "${BLUE}üöÄ Starting tool-enabled local proxy for Claude CLI integration...${NC}"
    
    # Check if proxy is already running
    if curl -s http://localhost:8001/health >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ Local proxy already running${NC}"
        return 0
    fi
    
    # Find the script directory and use static tool-enabled proxy
    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    
    if [ ! -f "$SCRIPT_DIR/local_tools_proxy.py" ]; then
        echo -e "${RED}‚ùå Tool-enabled proxy not found: $SCRIPT_DIR/local_tools_proxy.py${NC}"
        return 1
    fi
    
    # Set environment variables for the proxy
    export LM_STUDIO_HOST="$host"
    export LM_STUDIO_PORT="$port"
    export LM_STUDIO_MODEL="$model"
    export API_PORT="8001"
    
    # Start the tool-enabled proxy
    echo -e "${BLUE}‚è≥ Starting tool-enabled proxy server...${NC}"
    "$SCRIPT_DIR/claude_env/bin/python" "$SCRIPT_DIR/local_tools_proxy.py" > /tmp/lm_studio_proxy.log 2>&1 &
    PROXY_PID=$!
    
    # Wait for proxy to start
    for i in {1..10}; do
        sleep 1
        if curl -s http://localhost:8001/health >/dev/null 2>&1; then
            echo -e "${GREEN}‚úÖ Local proxy started successfully (PID: $PROXY_PID)${NC}"
            return 0
        fi
    done
    
    echo -e "${RED}‚ùå Failed to start local proxy${NC}"
    echo "Check /tmp/lm_studio_proxy.log for details"
    return 1
}

# Main function
main() {
    # Step 1: Discover Windows host
    show_host_discovery
    HOST_IP=$(discover_host)
    
    # Step 2: Check LM Studio
    if ! check_lm_studio "$HOST_IP" "$LM_STUDIO_PORT"; then
        echo -e "${RED}‚ùå LM Studio not accessible${NC}"
        echo -e "${YELLOW}üí° Make sure LM Studio is running on Windows with a model loaded${NC}"
        echo -e "${YELLOW}üí° Check that LM Studio server is enabled (port $LM_STUDIO_PORT)${NC}"
        exit 1
    fi
    
    # Step 3: Get the actual model name
    ACTUAL_MODEL=$(curl -s "http://${HOST_IP}:${LM_STUDIO_PORT}/v1/models" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    models = [m['id'] for m in data.get('data', [])]
    print(models[0] if models else 'unknown')
except:
    print('unknown')
" 2>/dev/null)
    
    if [ "$ACTUAL_MODEL" != "unknown" ]; then
        LM_STUDIO_MODEL="$ACTUAL_MODEL"
        echo -e "${GREEN}‚úÖ Using model: $LM_STUDIO_MODEL${NC}"
    fi
    
    # Step 4: Test connection
    if ! test_lm_studio "$HOST_IP" "$LM_STUDIO_PORT" "$LM_STUDIO_MODEL"; then
        echo -e "${YELLOW}‚ö†Ô∏è  Basic test failed, but continuing anyway...${NC}"
    fi
    
    # Step 5: Start local proxy
    if ! start_local_proxy "$HOST_IP" "$LM_STUDIO_PORT" "$LM_STUDIO_MODEL"; then
        echo -e "${RED}‚ùå Failed to start local proxy${NC}"
        exit 1
    fi
    
    echo ""
    echo -e "${GREEN}üéØ Ready to use Claude with LM Studio!${NC}"
    echo -e "${BLUE}üè† LM Studio: http://${HOST_IP}:${LM_STUDIO_PORT}${NC}"
    echo -e "${BLUE}üîó Proxy: http://localhost:8001${NC}"
    echo -e "${BLUE}üì± Model: ${LM_STUDIO_MODEL}${NC}"
    echo ""
    
    # Set environment and run Claude
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export ANTHROPIC_API_KEY="lm-studio"
    
    # Pass all arguments to claude
    if [ $# -eq 0 ]; then
        echo -e "${BLUE}ü§ñ Starting Claude in interactive mode...${NC}"
        echo -e "${YELLOW}‚è±Ô∏è  Note: First response may take 10-30 seconds${NC}"
        echo ""
        claude
    else
        echo -e "${BLUE}ü§ñ Running Claude with arguments: $*${NC}"
        echo ""
        claude "$@"
    fi
}

# Help message
if [[ "$1" == "--help" || "$1" == "-h" ]]; then
    echo "Claude CLI with Local LM Studio Integration"
    echo ""
    echo "What it does:"
    echo "  1. üîç Auto-discovers Windows host IP from WSL"
    echo "  2. ‚úÖ Validates LM Studio is running and accessible"
    echo "  3. üöÄ Starts local proxy to translate Anthropic‚ÜîOpenAI API formats"
    echo "  4. ü§ñ Runs Claude CLI with proper configuration"
    echo ""
    echo "Usage:"
    echo "  ./claude-local                    # Interactive mode"
    echo "  ./claude-local -p \"prompt\"        # Direct prompt"
    echo "  ./claude-local --help             # Show this help"
    echo ""
    echo "Configuration:"
    echo "  LM_STUDIO_PORT=${LM_STUDIO_PORT} (default: 1234)"
    echo "  LM_STUDIO_MODEL=${LM_STUDIO_MODEL}"
    echo ""
    echo "Requirements:"
    echo "  - LM Studio running on Windows with server enabled"
    echo "  - Qwen model loaded in LM Studio"
    echo "  - Python 3 with fastapi, uvicorn, requests installed"
    echo ""
    echo "Troubleshooting:"
    echo "  - Ensure LM Studio server is enabled (Local Server tab)"
    echo "  - Check Windows Firewall allows port ${LM_STUDIO_PORT}"
    echo "  - Verify WSL can reach Windows host"
    exit 0
fi

# Run main function
main "$@"