#!/bin/bash
# Claude CLI with Local LM Studio Integration
# Auto-discovers LM Studio on Windows host from WSL
# Usage: ./claude-local [claude options]

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}üè† Claude CLI - Local LM Studio Mode${NC}"
echo "========================================="

# Configuration
LM_STUDIO_PORT="${LM_STUDIO_PORT:-1234}"
LM_STUDIO_MODEL="${LM_STUDIO_MODEL:-qwen/qwen3-coder-30b}"

# Discover Windows host IP from WSL
discover_host() {
    # Check if we're in WSL
    if ! grep -q microsoft /proc/version 2>/dev/null; then
        echo "localhost"
        return 0
    fi
    
    # Get default gateway (Windows host IP)
    HOST_IP=$(ip route show | grep -i default | awk '{ print $3 }' | head -1)
    
    if [ -z "$HOST_IP" ]; then
        echo "localhost"
        return 1
    fi
    
    echo "$HOST_IP"
    return 0
}

# Display host discovery results
show_host_discovery() {
    echo -e "${BLUE}üîç Discovering Windows host from WSL...${NC}"
    
    HOST_IP=$(discover_host)
    
    if [ "$HOST_IP" == "localhost" ]; then
        if grep -q microsoft /proc/version 2>/dev/null; then
            echo -e "${YELLOW}‚ö†Ô∏è  Could not discover Windows host IP, using localhost${NC}"
        else
            echo -e "${YELLOW}‚ö†Ô∏è  Not running in WSL, using localhost${NC}"
        fi
    else
        echo -e "${GREEN}‚úÖ Found Windows host: $HOST_IP${NC}"
    fi
}

# Check if LM Studio is running and accessible
check_lm_studio() {
    local host=$1
    local port=$2
    
    echo -e "${BLUE}üîç Checking LM Studio at ${host}:${port}...${NC}"
    
    # Test basic connectivity
    echo -e "${BLUE}‚è≥ Testing connection to http://${host}:${port}/v1/models${NC}"
    if ! curl -s --connect-timeout 15 --max-time 30 "http://${host}:${port}/v1/models" >/dev/null 2>&1; then
        echo -e "${RED}‚ùå Cannot connect to LM Studio${NC}"
        echo -e "${YELLOW}üí° Testing direct connectivity...${NC}"
        curl -v --connect-timeout 5 "http://${host}:${port}/v1/models" 2>&1 | head -5
        return 1
    fi
    
    # Get models list
    MODELS_RESPONSE=$(curl -s --connect-timeout 10 "http://${host}:${port}/v1/models" 2>/dev/null)
    
    if [ $? -ne 0 ] || [ -z "$MODELS_RESPONSE" ]; then
        echo -e "${RED}‚ùå LM Studio not responding properly${NC}"
        return 1
    fi
    
    # Check if our expected model is loaded
    if echo "$MODELS_RESPONSE" | grep -q "qwen"; then
        MODEL_NAME=$(echo "$MODELS_RESPONSE" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    models = [m['id'] for m in data.get('data', [])]
    qwen_models = [m for m in models if 'qwen' in m.lower()]
    print(qwen_models[0] if qwen_models else 'unknown')
except:
    print('unknown')
" 2>/dev/null)
        
        echo -e "${GREEN}‚úÖ LM Studio is running${NC}"
        echo -e "${GREEN}üì± Available model: $MODEL_NAME${NC}"
        return 0
    else
        echo -e "${YELLOW}‚ö†Ô∏è  LM Studio running but no Qwen model loaded${NC}"
        echo -e "${BLUE}Available models:${NC}"
        echo "$MODELS_RESPONSE" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    for model in data.get('data', []):
        print(f\"  - {model['id']}\")
except:
    print('  Unable to parse models')
" 2>/dev/null
        return 1
    fi
}

# Test LM Studio connection with a simple query
test_lm_studio() {
    local host=$1
    local port=$2
    local model=$3
    
    echo -e "${BLUE}üß™ Testing LM Studio with simple query...${NC}"
    
    # Simple test query
    local test_response=$(curl -s --connect-timeout 15 \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer lm-studio" \
        -d "{
            \"model\": \"$model\",
            \"messages\": [{\"role\": \"user\", \"content\": \"Say 'Hello from LM Studio!' and nothing else.\"}],
            \"max_tokens\": 20,
            \"temperature\": 0.1
        }" \
        "http://${host}:${port}/v1/chat/completions" 2>/dev/null)
    
    if [ $? -eq 0 ] && echo "$test_response" | grep -q "Hello from LM Studio"; then
        echo -e "${GREEN}‚úÖ LM Studio test successful${NC}"
        return 0
    else
        echo -e "${YELLOW}‚ö†Ô∏è  LM Studio test failed or unexpected response${NC}"
        echo "Response: $test_response"
        return 1
    fi
}

# Create local proxy for Claude CLI integration
start_local_proxy() {
    local host=$1
    local port=$2
    local model=$3
    
    echo -e "${BLUE}üöÄ Starting local proxy for Claude CLI integration...${NC}"
    
    # Check if proxy is already running
    if curl -s http://localhost:8001/health >/dev/null 2>&1; then
        echo -e "${GREEN}‚úÖ Local proxy already running${NC}"
        return 0
    fi
    
    # Create temporary proxy script using http.server (built-in)
    cat > /tmp/lm_studio_proxy.py << EOF
#!/usr/bin/env python3
"""
LM Studio Proxy for Claude CLI Integration
Translates Anthropic API calls to LM Studio OpenAI-compatible API
Uses built-in Python http.server to avoid external dependencies
"""

import json
import requests
import time
import logging
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
import threading

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
LM_STUDIO_HOST = "${host}"
LM_STUDIO_PORT = ${port}
LM_STUDIO_MODEL = "${model}"
LM_STUDIO_BASE_URL = f"http://{LM_STUDIO_HOST}:{LM_STUDIO_PORT}/v1"

class ProxyHandler(BaseHTTPRequestHandler):
    def log_message(self, format, *args):
        """Override to use our logger"""
        logger.info(f"{self.address_string()} - {format % args}")
    
    def do_GET(self):
        """Handle GET requests"""
        parsed_path = urlparse(self.path)
        
        if parsed_path.path == "/health":
            self.handle_health()
        elif parsed_path.path == "/v1/models":
            self.handle_models()
        else:
            self.send_error(404, "Not Found")
    
    def do_POST(self):
        """Handle POST requests"""
        parsed_path = urlparse(self.path)
        
        if parsed_path.path == "/v1/messages":
            self.handle_messages()
        else:
            self.send_error(404, "Not Found")
    
    def handle_health(self):
        """Health check endpoint"""
        try:
            response = requests.get(f"{LM_STUDIO_BASE_URL}/models", timeout=5)
            if response.status_code == 200:
                health_data = {
                    "status": "healthy",
                    "lm_studio_proxy": "active",
                    "lm_studio_host": f"{LM_STUDIO_HOST}:{LM_STUDIO_PORT}",
                    "model": LM_STUDIO_MODEL,
                    "timestamp": time.time()
                }
                self.send_response(200)
                self.send_header('Content-Type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(health_data).encode())
                return
        except Exception as e:
            logger.error(f"Health check failed: {e}")
        
        self.send_error(503, "LM Studio not accessible")
    
    def handle_models(self):
        """List available models"""
        try:
            response = requests.get(f"{LM_STUDIO_BASE_URL}/models", timeout=10)
            self.send_response(response.status_code)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(response.content)
        except Exception as e:
            logger.error(f"Failed to get models: {e}")
            self.send_error(503, "LM Studio not accessible")
    
    def handle_messages(self):
        """Handle Anthropic-style messages and convert to OpenAI format"""
        try:
            content_length = int(self.headers['Content-Length'])
            request_body = self.rfile.read(content_length)
            data = json.loads(request_body.decode())
            
            logger.info(f"Received Claude request: {json.dumps(data, indent=2)}")
            
            # Convert Anthropic format to OpenAI format
            openai_request = {
                "model": LM_STUDIO_MODEL,
                "messages": data.get("messages", []),
                "max_tokens": data.get("max_tokens", 1000),
                "temperature": data.get("temperature", 0.7),
                "stream": False
            }
            
            # Make request to LM Studio
            response = requests.post(
                f"{LM_STUDIO_BASE_URL}/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": "Bearer lm-studio"
                },
                json=openai_request,
                timeout=60
            )
            
            if response.status_code != 200:
                logger.error(f"LM Studio error: {response.status_code} - {response.text}")
                self.send_error(response.status_code, response.text)
                return
            
            openai_response = response.json()
            logger.info(f"LM Studio response received")
            
            # Convert OpenAI response to Anthropic format
            content = openai_response["choices"][0]["message"]["content"]
            
            anthropic_response = {
                "id": f"msg_{int(time.time())}",
                "type": "message",
                "role": "assistant",
                "content": [{"type": "text", "text": content}],
                "model": LM_STUDIO_MODEL,
                "stop_reason": "end_turn",
                "stop_sequence": None,
                "usage": {
                    "input_tokens": openai_response.get("usage", {}).get("prompt_tokens", 0),
                    "output_tokens": openai_response.get("usage", {}).get("completion_tokens", 0)
                }
            }
            
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps(anthropic_response).encode())
            
        except Exception as e:
            logger.error(f"Error processing request: {e}")
            self.send_error(500, str(e))

def run_server():
    """Run the proxy server"""
    server = HTTPServer(('0.0.0.0', 8001), ProxyHandler)
    logger.info("Starting LM Studio proxy server on port 8001")
    server.serve_forever()

if __name__ == "__main__":
    run_server()
EOF
    
    # Start proxy in background
    echo -e "${BLUE}‚è≥ Starting proxy server...${NC}"
    python3 /tmp/lm_studio_proxy.py > /tmp/lm_studio_proxy.log 2>&1 &
    PROXY_PID=$!
    
    # Wait for proxy to start
    for i in {1..10}; do
        sleep 1
        if curl -s http://localhost:8001/health >/dev/null 2>&1; then
            echo -e "${GREEN}‚úÖ Local proxy started successfully (PID: $PROXY_PID)${NC}"
            return 0
        fi
    done
    
    echo -e "${RED}‚ùå Failed to start local proxy${NC}"
    echo "Check /tmp/lm_studio_proxy.log for details"
    return 1
}

# Main function
main() {
    # Step 1: Discover Windows host
    show_host_discovery
    HOST_IP=$(discover_host)
    
    # Step 2: Check LM Studio
    if ! check_lm_studio "$HOST_IP" "$LM_STUDIO_PORT"; then
        echo -e "${RED}‚ùå LM Studio not accessible${NC}"
        echo -e "${YELLOW}üí° Make sure LM Studio is running on Windows with a model loaded${NC}"
        echo -e "${YELLOW}üí° Check that LM Studio server is enabled (port $LM_STUDIO_PORT)${NC}"
        exit 1
    fi
    
    # Step 3: Get the actual model name
    ACTUAL_MODEL=$(curl -s "http://${HOST_IP}:${LM_STUDIO_PORT}/v1/models" | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    models = [m['id'] for m in data.get('data', [])]
    print(models[0] if models else 'unknown')
except:
    print('unknown')
" 2>/dev/null)
    
    if [ "$ACTUAL_MODEL" != "unknown" ]; then
        LM_STUDIO_MODEL="$ACTUAL_MODEL"
        echo -e "${GREEN}‚úÖ Using model: $LM_STUDIO_MODEL${NC}"
    fi
    
    # Step 4: Test connection
    if ! test_lm_studio "$HOST_IP" "$LM_STUDIO_PORT" "$LM_STUDIO_MODEL"; then
        echo -e "${YELLOW}‚ö†Ô∏è  Basic test failed, but continuing anyway...${NC}"
    fi
    
    # Step 5: Start local proxy
    if ! start_local_proxy "$HOST_IP" "$LM_STUDIO_PORT" "$LM_STUDIO_MODEL"; then
        echo -e "${RED}‚ùå Failed to start local proxy${NC}"
        exit 1
    fi
    
    echo ""
    echo -e "${GREEN}üéØ Ready to use Claude with LM Studio!${NC}"
    echo -e "${BLUE}üè† LM Studio: http://${HOST_IP}:${LM_STUDIO_PORT}${NC}"
    echo -e "${BLUE}üîó Proxy: http://localhost:8001${NC}"
    echo -e "${BLUE}üì± Model: ${LM_STUDIO_MODEL}${NC}"
    echo ""
    
    # Set environment and run Claude
    export ANTHROPIC_BASE_URL="http://localhost:8001"
    export ANTHROPIC_API_KEY="lm-studio"
    
    # Pass all arguments to claude
    if [ $# -eq 0 ]; then
        echo -e "${BLUE}ü§ñ Starting Claude in interactive mode...${NC}"
        echo -e "${YELLOW}‚è±Ô∏è  Note: First response may take 10-30 seconds${NC}"
        echo ""
        claude
    else
        echo -e "${BLUE}ü§ñ Running Claude with arguments: $*${NC}"
        echo ""
        claude "$@"
    fi
}

# Help message
if [[ "$1" == "--help" || "$1" == "-h" ]]; then
    echo "Claude CLI with Local LM Studio Integration"
    echo ""
    echo "What it does:"
    echo "  1. üîç Auto-discovers Windows host IP from WSL"
    echo "  2. ‚úÖ Validates LM Studio is running and accessible"
    echo "  3. üöÄ Starts local proxy to translate Anthropic‚ÜîOpenAI API formats"
    echo "  4. ü§ñ Runs Claude CLI with proper configuration"
    echo ""
    echo "Usage:"
    echo "  ./claude-local                    # Interactive mode"
    echo "  ./claude-local -p \"prompt\"        # Direct prompt"
    echo "  ./claude-local --help             # Show this help"
    echo ""
    echo "Configuration:"
    echo "  LM_STUDIO_PORT=${LM_STUDIO_PORT} (default: 1234)"
    echo "  LM_STUDIO_MODEL=${LM_STUDIO_MODEL}"
    echo ""
    echo "Requirements:"
    echo "  - LM Studio running on Windows with server enabled"
    echo "  - Qwen model loaded in LM Studio"
    echo "  - Python 3 with fastapi, uvicorn, requests installed"
    echo ""
    echo "Troubleshooting:"
    echo "  - Ensure LM Studio server is enabled (Local Server tab)"
    echo "  - Check Windows Firewall allows port ${LM_STUDIO_PORT}"
    echo "  - Verify WSL can reach Windows host"
    exit 0
fi

# Run main function
main "$@"